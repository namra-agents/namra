# Namra Architecture Overview

**Last Updated**: January 28, 2026
**Status**: Week 2 Complete (MVP 50% done)

---

## High-Level Architecture

Namra is a **config-driven agent framework** with a **Rust core runtime**. The key design principle is: **define agents in YAML, execute in Rust**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Layer                              â”‚
â”‚  â€¢ YAML Configuration Files                                     â”‚
â”‚  â€¢ CLI Commands (namra init, validate, run)                     â”‚
â”‚  â€¢ Future: Python SDK, REST API                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Namra Core (Rust)                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ namra-config â”‚  â”‚  namra-cli   â”‚  â”‚  namra-llm   â”‚        â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚        â”‚
â”‚  â”‚ â€¢ YAML Parse â”‚  â”‚ â€¢ init       â”‚  â”‚ â€¢ Anthropic  â”‚        â”‚
â”‚  â”‚ â€¢ Validation â”‚  â”‚ â€¢ validate   â”‚  â”‚ â€¢ Streaming  â”‚        â”‚
â”‚  â”‚ â€¢ Types      â”‚  â”‚ â€¢ run        â”‚  â”‚ â€¢ Cost Track â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â†“                  â†“                  â†“                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚           namra-runtime (Week 4)                 â”‚         â”‚
â”‚  â”‚  â€¢ Agent Executor                                â”‚         â”‚
â”‚  â”‚  â€¢ ReAct Strategy Loop                           â”‚         â”‚
â”‚  â”‚  â€¢ Tool Calling Orchestration                    â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â†“                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ namra-tools  â”‚  â”‚ namra-memory â”‚  â”‚ namra-mdware â”‚        â”‚
â”‚  â”‚ (Week 3)     â”‚  â”‚ (Week 12)    â”‚  â”‚ (Week 9-11)  â”‚        â”‚
â”‚  â”‚ â€¢ HTTP       â”‚  â”‚ â€¢ In-memory  â”‚  â”‚ â€¢ Observ.    â”‚        â”‚
â”‚  â”‚ â€¢ Filesystem â”‚  â”‚ â€¢ Redis      â”‚  â”‚ â€¢ Security   â”‚        â”‚
â”‚  â”‚ â€¢ Calculator â”‚  â”‚ â€¢ Vector DB  â”‚  â”‚ â€¢ Govern.    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      External Services                          â”‚
â”‚  â€¢ Anthropic API (Claude)                                       â”‚
â”‚  â€¢ Future: OpenAI, Google, Bedrock                              â”‚
â”‚  â€¢ Storage: Redis, PostgreSQL, Vector DBs                       â”‚
â”‚  â€¢ Observability: Jaeger, Prometheus                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Components (Implemented)

### 1. **namra-config** âœ… Complete
**Purpose**: Parse and validate agent configuration files

**Responsibilities**:
- Parse YAML/TOML configuration files
- Validate agent definitions (LLM settings, tools, memory, etc.)
- Type-safe configuration structs
- Error reporting with context

**Key Types**:
```rust
pub struct AgentConfig {
    pub name: String,
    pub version: String,
    pub llm: LLMConfig,
    pub tools: Vec<ToolConfig>,
    pub memory: MemoryConfig,
    pub execution: ExecutionConfig,
    pub system_prompt: String,
}
```

**Status**:
- ~800 LOC
- 4 unit tests
- Supports YAML and TOML

---

### 2. **namra-cli** âœ… Complete
**Purpose**: Command-line interface for agent operations

**Commands Implemented**:
- `namra init <name>` - Scaffold new agent project
- `namra validate <file>` - Validate configuration
- `namra run <file>` - Execute agent with LLM
- `namra version` - Show version info

**Features**:
- Colored output (success/error highlighting)
- Progress indicators for long operations
- Helpful error messages
- Environment variable support (ANTHROPIC_API_KEY)

**Example Usage**:
```bash
# Initialize project
namra init my-agent
cd my-agent

# Validate config
namra validate agents/example_agent.yaml

# Run agent (non-streaming)
namra run agents/example_agent.yaml --input "Hello!"

# Run agent (streaming)
namra run agents/example_agent.yaml --input "Tell me a story" --stream
```

**Status**:
- ~700 LOC
- Async runtime (Tokio)
- Clap for CLI parsing

---

### 3. **namra-llm** âœ… Complete
**Purpose**: LLM provider integrations

**Architecture**:
```rust
// Trait for all LLM providers
#[async_trait]
pub trait LLMAdapter: Send + Sync {
    async fn generate(&self, request: LLMRequest) -> Result<LLMResponse>;
    async fn stream(&self, request: LLMRequest) -> Result<LLMStream>;
    fn provider_name(&self) -> &str;
    fn estimate_cost(&self, tokens: &TokenUsage) -> f64;
}

// Currently implemented
pub struct AnthropicAdapter {
    client: reqwest::Client,
    api_key: String,
    model: String,
}
```

**Features**:
- **Non-streaming mode**: Wait for complete response
- **Streaming mode**: Real-time SSE (Server-Sent Events)
- **Cost calculation**: Automatic per-request cost tracking
- **Token usage**: Input/output token counting
- **Error handling**: 401 (auth), 429 (rate limit), 400 (validation), 500 (server errors)
- **Model support**: Claude Sonnet 4.5, Sonnet 3.5, Opus, Haiku

**Key Types**:
```rust
pub struct LLMRequest {
    pub messages: Vec<Message>,
    pub model: String,
    pub temperature: f32,
    pub max_tokens: u32,
    pub stream: bool,
}

pub struct LLMResponse {
    pub content: String,
    pub finish_reason: FinishReason,
    pub usage: TokenUsage,
}

pub struct TokenUsage {
    pub input_tokens: u32,
    pub output_tokens: u32,
}
```

**Cost Calculation**:
```rust
// Claude Sonnet 4.5 pricing
Input:  $3.00 / 1M tokens
Output: $15.00 / 1M tokens

// Example: 1000 input + 500 output tokens
Cost = (1000/1M Ã— $3) + (500/1M Ã— $15) = $0.0105
```

**Status**:
- ~1,400 LOC
- 5 unit tests (1 ignored integration test)
- Full Anthropic integration
- OpenAI adapter deferred to Week 5+

---

## Data Flow: Current System

### Scenario: User runs an agent

```
1. USER INPUT
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ $ namra run agent.yaml --input "Hi" â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
2. CLI PARSING (namra-cli)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ Parse command-line args           â”‚
   â”‚ â€¢ Read agent.yaml file              â”‚
   â”‚ â€¢ Extract --input parameter         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
3. CONFIG LOADING (namra-config)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ Parse YAML â†’ AgentConfig struct   â”‚
   â”‚ â€¢ Validate LLM settings             â”‚
   â”‚ â€¢ Validate tools, memory, etc.      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
4. LLM ADAPTER SETUP (namra-llm)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ Read ANTHROPIC_API_KEY            â”‚
   â”‚ â€¢ Initialize AnthropicAdapter       â”‚
   â”‚ â€¢ Set model, temperature, max_tokensâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
5. LLM REQUEST (namra-llm)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ â€¢ Build LLMRequest:                 â”‚
   â”‚   - messages: [system, user]        â”‚
   â”‚   - model: claude-sonnet-4-5-...    â”‚
   â”‚   - temperature: 0.7                â”‚
   â”‚ â€¢ POST to Anthropic API             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
6. ANTHROPIC API
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ POST https://api.anthropic.com/v1/  â”‚
   â”‚      messages                        â”‚
   â”‚                                      â”‚
   â”‚ Headers:                             â”‚
   â”‚   x-api-key: sk-ant-...             â”‚
   â”‚   anthropic-version: 2023-06-01     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
7. STREAMING RESPONSE (if --stream)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ event: message_start                â”‚
   â”‚ event: content_block_start          â”‚
   â”‚ event: content_block_delta          â”‚
   â”‚   data: {"text": "Hello"}           â”‚
   â”‚ event: content_block_delta          â”‚
   â”‚   data: {"text": " there"}          â”‚
   â”‚ event: message_stop                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
8. OUTPUT RENDERING (namra-cli)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Agent:                              â”‚
   â”‚ Hello there! How can I help you?    â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
   â”‚ Tokens: 23 tokens (in: 15, out: 8) â”‚
   â”‚ Cost: $0.0002                       â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Details

### Configuration System (namra-config)

**File Format**:
```yaml
name: example_agent
version: 1.0.0

llm:
  provider: anthropic                    # â† LLM provider
  model: claude-sonnet-4-5-20250929     # â† Model name
  temperature: 0.7                       # â† Randomness (0-1)
  max_tokens: 4096                       # â† Response length limit

tools:                                   # â† Available tools (Week 3+)
  - name: web_search
    type: builtin.http
    config:
      url: https://api.tavily.com/search

memory:                                  # â† Conversation memory (Week 12+)
  type: in_memory
  max_messages: 100

execution:                               # â† Execution strategy (Week 4+)
  strategy: react                        # â† ReAct pattern
  max_iterations: 10                     # â† Stop after N loops
  timeout: 60s                           # â† Total timeout

system_prompt: |                         # â† System instructions
  You are a helpful assistant.
```

**Validation Rules**:
- `temperature`: Must be between 0.0 and 1.0
- `max_tokens`: Must be positive, typically â‰¤ 100,000
- `provider`: Must be valid (anthropic, openai, etc.)
- `model`: Must exist for the provider
- `tools`: Must reference valid tool types
- `execution.strategy`: Must be valid (react, chain, etc.)

---

### LLM Adapter (namra-llm)

**Request Flow**:
```rust
// 1. Create adapter
let adapter = AnthropicAdapter::builder()
    .api_key(env::var("ANTHROPIC_API_KEY")?)
    .model("claude-sonnet-4-5-20250929")
    .build();

// 2. Build request
let request = LLMRequest {
    messages: vec![
        Message::system("You are a helpful assistant"),
        Message::user("What is 2+2?"),
    ],
    model: "claude-sonnet-4-5-20250929".to_string(),
    temperature: 0.7,
    max_tokens: 1024,
    stream: false,
};

// 3. Generate response
let response = adapter.generate(request).await?;

// 4. Access result
println!("Response: {}", response.content);
println!("Cost: ${:.4}", adapter.estimate_cost(&response.usage));
```

**Streaming Flow**:
```rust
// 1. Create streaming request
let request = LLMRequest {
    stream: true,
    // ... other fields
};

// 2. Get stream
let mut stream = adapter.stream(request).await?;

// 3. Process chunks
while let Some(chunk) = stream.next().await {
    match chunk? {
        StreamChunk::Content(text) => print!("{}", text),
        StreamChunk::Done(usage) => {
            println!("\n\nTokens: {}", usage.input_tokens + usage.output_tokens);
        }
    }
}
```

**Error Handling**:
```rust
pub enum LLMError {
    Unauthorized,           // 401 - Invalid API key
    RateLimited,           // 429 - Too many requests
    InvalidRequest(String), // 400 - Bad parameters
    ModelNotFound,         // 404 - Model doesn't exist
    ServerError,           // 500+ - Anthropic issue
    NetworkError(String),  // Connection problems
}
```

---

## Stub Components (Not Yet Implemented)

### 4. **namra-tools** ğŸš§ Week 3
**Purpose**: Built-in tools for agents

**Planned Tools**:
- **HTTP Tool**: Make REST API calls
- **File System Tool**: Read/write files
- **Calculator Tool**: Basic arithmetic
- **Database Tool**: SQL queries
- **Shell Tool**: Execute commands

**Interface**:
```rust
#[async_trait]
pub trait Tool: Send + Sync {
    fn name(&self) -> &str;
    fn description(&self) -> &str;
    fn parameters(&self) -> serde_json::Value; // JSON Schema
    async fn execute(&self, input: serde_json::Value) -> Result<ToolOutput>;
}
```

---

### 5. **namra-runtime** ğŸ“… Week 4 (MVP Goal)
**Purpose**: Agent execution engine

**Responsibilities**:
- Execute agents using ReAct pattern (Reasoning + Action loop)
- Manage conversation history
- Call tools based on LLM decisions
- Handle stop conditions (max iterations, timeout)
- Error recovery

**ReAct Loop**:
```
1. Agent receives task
2. THINK: LLM reasons about what to do
3. ACT: LLM decides to use a tool (or respond)
4. OBSERVE: Execute tool, get result
5. Repeat steps 2-4 until task complete
6. Return final answer
```

**Example**:
```
User: "What's the weather in San Francisco?"

Iteration 1:
  THINK: "I need to check the weather, I'll use the web_search tool"
  ACT: Call web_search("weather San Francisco")
  OBSERVE: "Current temp: 65Â°F, Sunny"

Iteration 2:
  THINK: "I have the weather info, I can answer now"
  ACT: Respond to user
  OUTPUT: "It's currently 65Â°F and sunny in San Francisco!"
```

---

### 6. **namra-memory** ğŸ“… Week 12
**Purpose**: Conversation and context storage

**Implementations**:
- **In-Memory**: Simple Vec<Message> storage
- **Redis**: Persistent key-value storage
- **PostgreSQL**: Relational storage with search
- **Vector DB**: Semantic search (Pinecone, Weaviate, Qdrant)

---

### 7. **namra-middleware** ğŸ“… Weeks 9-11
**Purpose**: Cross-cutting concerns

**Types**:
- **Observability**: OpenTelemetry tracing, metrics, logs
- **Security**: Input validation, rate limiting, auth
- **Governance**: Policy enforcement, cost tracking, compliance

---

### 8. **namra-api** ğŸ“… Week 5
**Purpose**: gRPC/HTTP server for remote execution

**Endpoints**:
- `POST /v1/agents/execute` - Run agent
- `POST /v1/agents/validate` - Validate config
- `GET /v1/agents/{id}/status` - Check status

---

### 9. **namra-plugin** ğŸ“… Week 7
**Purpose**: Python custom tool integration

Allows users to write custom tools in Python:
```python
from namra import tool

@tool
def custom_search(query: str) -> str:
    """Search internal knowledge base"""
    # Custom logic here
    return results
```

---

## Design Principles

### 1. **Config-Driven Architecture**
- **80% use cases**: No code required, just YAML
- **20% use cases**: Custom tools via Python plugins
- **Benefits**:
  - Non-programmers can create agents
  - Version control configurations (GitOps)
  - No recompilation needed

### 2. **Performance-First**
- **Rust core**: 10-100Ã— faster than Python
- **Zero-copy parsing**: Efficient memory usage
- **Async everywhere**: Tokio for concurrency
- **Streaming**: Real-time responses

### 3. **LLM Agnostic**
- Unified `LLMAdapter` trait
- Easy to add new providers
- Swap providers via config only

### 4. **Enterprise-Native**
- Observability built-in (not bolted-on)
- Security by default
- Multi-tenancy support
- Cost tracking

### 5. **Production-Ready**
- Single binary deployment
- Kubernetes-ready (but not required)
- Graceful shutdown
- Circuit breakers and retries

---

## Technology Stack

### Core
- **Language**: Rust 1.75+
- **Async Runtime**: Tokio
- **Serialization**: Serde (JSON, YAML, TOML)
- **HTTP Client**: Reqwest
- **CLI Framework**: Clap

### Future
- **gRPC**: Tonic
- **HTTP Server**: Axum
- **Observability**: OpenTelemetry
- **Testing**: Mockito, Criterion (benchmarks)

---

## Performance Characteristics

### Current (Week 2)
- **Binary Size**: ~12 MB (debug), ~6 MB (release)
- **Cold Start**: <5ms for CLI commands
- **Memory**: <10 MB for CLI operations
- **Compile Time**: ~25s (clean), ~2s (incremental)
- **LLM Latency**: ~500-2000ms (network-bound, depends on Anthropic)

### Target (MVP - Week 4)
- **Cold Start**: <20ms for agent execution
- **Memory**: <50MB per agent instance
- **Throughput**: 100+ requests/second (single instance)
- **Binary Size**: <10MB (release)

---

## Comparison to Similar Systems

| Feature | Namra | LangChain | AutoGPT | Agno |
|---------|-------|-----------|---------|------|
| **Language** | Rust | Python | Python | Python |
| **Config** | YAML | Code | JSON | Code |
| **Cold Start** | <20ms | ~500ms | ~1000ms | ~150ms |
| **Memory** | <50MB | ~300MB | ~500MB | ~80MB |
| **LLM Agnostic** | âœ… | âœ… | âš ï¸ OpenAI-first | âœ… |
| **Streaming** | âœ… | âœ… | âŒ | âœ… |
| **Tools** | Built-in + Python | Python only | Python only | Python + Go |
| **Observability** | Built-in | 3rd party | Limited | Basic |
| **Multi-Agent** | âœ… (Week 13) | âœ… | âœ… | âš ï¸ |

---

## Security Considerations

### Current (Week 2)
- âœ… API keys from environment variables (not hardcoded)
- âœ… HTTPS for all LLM API calls
- âŒ No input sanitization yet
- âŒ No rate limiting yet
- âŒ No secret management yet

### Planned (Week 9-11)
- Input validation and sanitization
- Rate limiting (per-user, per-agent)
- Secret management (HashiCorp Vault integration)
- RBAC (Role-Based Access Control)
- Audit logging

---

## What's Next: Week 3 & 4

### Week 3: Tools
```
namra-tools/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ tool.rs          # Tool trait
â”‚   â”œâ”€â”€ http.rs          # HTTP tool
â”‚   â”œâ”€â”€ filesystem.rs    # File operations
â”‚   â””â”€â”€ builtin.rs       # Calculator, etc.
```

**Goal**: Agents can perform actions (not just chat)

### Week 4: Runtime (MVP!)
```
namra-runtime/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ executor.rs      # Agent executor
â”‚   â”œâ”€â”€ context.rs       # Execution context
â”‚   â””â”€â”€ strategies/
â”‚       â””â”€â”€ react.rs     # ReAct pattern
```

**Goal**: Complete agent loop with tool calling

**MVP Demo** (End of Week 4):
```bash
$ namra run research_agent.yaml \
  --input "Find the latest Rust news and summarize"

[Agent] Thinking: I need to search for Rust news
[Agent] Action: Using web_search tool...
[Tool] Found 5 articles about Rust
[Agent] Thinking: I'll summarize these articles
[Agent] Response: Here's a summary of the latest Rust news...

Cost: $0.045
Time: 3.2s
Tokens: 1,234
```

---

## Project Timeline

```
Week 1-2:  Foundation (Config, CLI, LLM)          âœ… DONE
Week 3:    Tools (HTTP, Filesystem, Calculator)   ğŸš§ IN PROGRESS
Week 4:    Runtime (ReAct, Tool Loop)             ğŸ“… NEXT WEEK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Week 5-8:  API & Python SDK                       ğŸ“… FUTURE
Week 9-12: Observability & Resilience            ğŸ“… FUTURE
Week 13-16: Multi-Agent & Workflows              ğŸ“… FUTURE
Week 17-20: Enterprise & Polish                  ğŸ“… FUTURE
```

**MVP Target**: End of Week 4
**Production-Ready Target**: End of Week 20

---

## Key Files

| File | Purpose |
|------|---------|
| `Cargo.toml` | Workspace definition |
| `PROJECT_STATUS.md` | Current status, metrics |
| `ROADMAP.md` | 20-week implementation plan |
| `NEXT_STEPS.md` | Step-by-step guide for current week |
| `WEEK2_COMPLETE.md` | Week 2 summary |
| `README.md` | Project overview |
| `ARCHITECTURE.md` | This file - system architecture |

---

**For detailed implementation**: See [NEXT_STEPS.md](NEXT_STEPS.md)
**For project timeline**: See [ROADMAP.md](ROADMAP.md)
**For current status**: See [PROJECT_STATUS.md](PROJECT_STATUS.md)
